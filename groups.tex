% !TEX encoding = MacOSRoman
% Standalone document
\documentclass[notes.tex]{subfiles}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Groups and algebras}
\label{chap:groups}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The study of symmetries plays a central part in theoretical physics, and the mathematical language we use is that of groups. The action of the group elements on our (quantum) states effects the transformation that has the symmetry, while the invariance of the physical properties of the system under that transformation is the symmetry itself. For example, rotations in three-dimensions can be carried out by the application of a $3\times3$ rotation matrix on the coordinates of an object. As we will see, these matrices form the group called $SO(3)$. For a sphere, which is invariant under these rotations, $SO(3)$ is then the symmetry group.

Of special interest to us are the Lie groups, which are the groups that represent continuous transformations, such as the $SO(3)$ rotations. The properties of Lie groups can be further studied by finding their generators which form a (Lie) algebra. The generators {\it almost} -- in a very specific sense of the word almost -- describes the whole group, and allows us to reconstruct the group elements by what is called the exponential map.

Here, we will begin by defining groups and looking at some of their most important properties. What is crucial in physics are the {\it representations} of groups, meaning what the operators of the transformations on the states actually look like. Returning to the rotation example these are $3\times3$ matrices, but with some restrictions on their elements. After discussing representations we will move on to defining Lie groups, before we end on a discussion of their generators and corresponding algebras.

%%%%%%%%%%%%%%
\section{Group definition}
%%%%%%%%%%%%%%
A group is an abstract mathematical structure that consists of a set of objects (elements), and a multiplication rule acting between pairs of these objects. We define a group as follows.
\df{The set of elements $G = \{g_i\}$ and operation $\circ$ (sometimes called multiplication) form a {\bf group} if and only if for all $g_i \in G$ :\begin{enumerate}[i)]
\item $g_i \circ g_j \in G$, \hfill (closure)
\item $(g_i \circ g_j)\circ g_k = g_i\circ(g_j \circ g_k)$, \hfill (associativity)
\item $\exists e \in G$ such that $g_i \circ e = e\circ g_i = g_i$, \hfill (identity element)
\item $\exists g_i^{-1} \in G$ such that $g_i \circ g_i^{-1} = g_i^{-1}\circ g_i = e$. \hfill (inverse)
\end{enumerate}}
Below, where no confusion can occur, we will often drop the multiplication symbol for the group multiplication (and other abstract multiplications), writing $g_i \circ g_j =g_ig_j$.
 
A straight forward example of a group is $G= \mathbb{Z}$ (the integers), with standard addition as the operation $\circ$. Then $e = 0$ and $g^{-1} = -g$. Alternatively we can restrict the group to $\mathbb{Z}_n$, where the operation is now addition modulo $n$. In this group, $g_i^{-1} = n - g_i$ and the unit element is again $e = 0$.\footnote{Note that we here use $e$ for the identity in an abstract group, while we will later use $I$ or $1$ as the identity matrix in matrix representations of groups.} 
Here, $\mathbb{Z}$ is an example of an {\bf infinite} group, the set has an infinite number of members, while $\mathbb{Z}_n$ is {\bf finite}, with {\bf order} $n$, meaning $n$ members. Both are {\bf abelian} groups, meaning that the elements commute: $g_i \circ g_j = g_j \circ g_i$, because the standard addition commutes.

The simplest, non-trivial, of these $\mathbb{Z}_n$-groups is $\mathbb{Z}_2$ which only has the members $e=0$ and $1$. The ``multiplication'' operation is completely defined by the three possibilities $0+0=0$, $0+1=1$ and $1+1=0$. Now, compare this to the set $G=\{-1,1\}$ with the ordinary multiplication operation. Here, all the possible operations are $1\cdot 1=1$, $1\cdot (-1)=-1$ and $(-1)\cdot (-1)=1$. This has exactly the same structure as $\mathbb{Z}_2$, only that the identity element is now $1$.  We say that these two groups are {\bf isomorphic}, because there is a one-to-one correspondence between all the (two) elements, $0\leftrightarrow 1$ and $1\leftrightarrow -1$, and the results of the multiplication operation is the same, and in fact we consider them as the {\it same group} despite the considerable apparent visual differences.\footnote{This observation  generalises to the set $G=\{e^{2\pi ik/n} | k=1,\ldots,n-1\}$, the {\bf $\mathbf n$-th roots of unity}, which, together with the standard multiplication operation, is isomorphic to  $\mathbb{Z}_n$.} 
This notion of isomorphic (``identical'') groups is very important, and we will return to it in more detail in Sec.~\ref{sec:group_properties}.

A somewhat more sophisticated example of a group can be found in the Taylor expansion of a function $F$, where
\begin{eqnarray*}
F(x+a) &=& F(x) + aF'(x) + \frac{1}{2} a^2 F''(x) + \ldots \\
&=& \sum_{n=0}^{\infty}\frac{a^n}{n!}\frac{\partial^n}{\partial x^n} F(x)\\
&=& e^{a\frac{\partial}{\partial x}} F(x).
\end{eqnarray*}
The last equality uses the formal definition of the exponential series, but may drive some mathematicians crazy.\footnote{We will not discuss this further, but there is a deep question here whether the operator formed by this exponentiation is well defined.}  The resulting operator $T_a = e^{a\frac{\partial}{\partial x}} $ is called the {\bf translation operator}, in this case in one dimension, since it shifts the coordinate $x$ of the function $F$ it is operating on by an amount $a$. Defining the (natural) multiplication operation
$T_a \circ T_b = T_{a+b}$
it forms the {\bf translational group} $T(1)$, where we can show that $T_0$ is the identity element and the inverse is $T_a^{-1} = T_{-a}$.\footnote{We could instead have defined the operation between two group elements to be ordinary multiplication and used that to show the relationship $T_a \circ T_b = T_{a+b}$. However, it is important to notice that showing this is not entirely trivial because ordinary arithmetic rules for exponentials fail for operators. In this particular case the proof is fairly simple, but this is in general not so. This will return to trouble us later in the notes.} In $n$ dimensions the group $T(n)$ has the elements  $T_{\mathbf{a}} = e^{\mathbf{a}\cdot \mathbf{\nabla}}$. Whereas we say that the groups $\mathbb{Z}$ and $\mathbb{Z}_n$ are {\bf discrete groups}, since we can count the number of elements, $T_a$ is a {\bf continuous group} since the parameter $a$ can be any real number. 


%%%%%%%%%%%%%%
\section{Matrix groups}
\label{sec:matrix_groups}
%%%%%%%%%%%%%%
We next define some groups that are very important in physics and to the discussion in these notes. They have in common that they are defined in terms of square matrices.

%%%%
\subsection{General and special linear groups}
The largest matrix group for a given matrix dimension $n$ is the general linear group.
\df{The {\bf general linear group} $GL(n)$ is defined by the set of all invertible $n\times n$ matrices $A$ under matrix multiplication. If we additionally require that $\det(A) = 1$, the matrices form the {\bf special linear group} $SL(n)$.}
Th closure property of groups is guaranteed by the fact that the product of two invertible matrices is also invertible, and matrix multiplication is always associative.
The existence of the group identity is guaranteed by the identity matrix $I$ being an invertible matrix (with $I$ as the inverse). Since the existence of an inverse is also necessary in the group definition, we can not construct larger matrix groups. The general linear group also give us our first example of a {\bf non-abelian group}, since matrix multiplication does not in general commute. For two matrices $A$ and $B$, we may have $AB\ne BA$.

We usually take the matrices in matrix groups to be defined over the field of complex numbers $\mathbb{C}$. If we want to specify the field we may use the notation $GL(n, \mathbb{R})$, signifying that the group is defined over the real numbers. Defined over the complex numbers the $GL(n)$ groups have $2n^2$ free parameters since each of the $n^2$ elements of the matrices can be a complex number, needing two parameters. The $SL(n)$ group has $2n^2-2$ free parameters since the requirement on the determinant fixes both the real and imaginary part of the determinant.

%%%%
\subsection{Unitary and special unitary groups}
\label{sec:unitary_groups}
We first remind you that the {\bf Hermitian conjugate} or conjugate transpose of a matrix is given by transposing the matrix and taking the complex conjugate of its elements. Here, we will use the dagger symbol $\dagger$ for this operation, so that for a matrix $A$, $A^\dagger=(A^T)^*$.

We now define the unitary groups.
\df{The {\bf unitary group} $U(n)$ is defined by the set of complex unitary $n\times n$ matrices $U$,
{\it i.e.}\ matrices such that $U^\dagger U = I$ or $U^{-1} = U^{\dagger}$. If we additionally require that $\det U = 1$ the matrices form the {\bf special unitary group} $SU(n)$.}

Since for $U\in U(n)$,
\[\det(UU^\dagger)=\det(U)\det(U^\dagger)=\det(U)\det(U^T)^*=\det(U)\det(U)^*=\det(I)=1,\] 
we have that the determinant of these matrices must be complex numbers on the unit circle, {\it i.e.}\ $\det(U)=e^{i\theta}$.
It can be shown, see Ex.~\ref{ex:Un_parameters}, that the $U(n)$, groups have $n^2$ independent parameters, while the $SU(n)$ groups have $n^2-1$.

It is these unitary groups that form the gauge symmetry groups of the Standard Model: $SU(3)$, $SU(2)$ and $U(1)$. The group $U(1)$ makes perfect sense despite the odd matrix dimensions. This is simply the set of all complex numbers of unit length with ordinary multiplication, {\it i.e.}\ $U(1)=\{e^{i\alpha} | \alpha \in\mathbb{R} \}$, but notice that $SU(1)$ would be trivial since it contains only the element 1. 

The members of the unitary group has has the important property that for all complex vectors $\mathbf{x}, \mathbf{y} \in \mathbb{C}^n$ (think finite-dimensional quantum states)  multiplication by  a unitary matrix leaves scalar products (inner products) unchanged. If $\mathbf{x}'=U\mathbf{x}$ and $\mathbf{y}'=U\mathbf{y}$, then
\begin{eqnarray}
\mathbf{x}'\cdot \mathbf{y}' &\equiv& \mathbf{x}'{}^\dagger \mathbf{y}' = (U\mathbf{x})^\dagger U\mathbf{y}\nonumber\\
&=& \mathbf{x}^\dagger U^\dagger U\mathbf{y} = \mathbf{x}^\dagger \mathbf{y} = \mathbf{x} \cdot \mathbf{y}.\nonumber
\end{eqnarray}
Thus, as this implies $|\mathbf{x}'|=|\mathbf{x}|$, its members do not change the length of the vectors they act on.  Since we would like to let our group representations act on vectors that describe quantum mechanical states, the unitary groups then conserve probability for these states. For example, when acting on a complex number (a complex scalar), such as a wavefunction $\psi(x)$, the elements of $U(1)$ rotate the phase of $\psi$, however, the magnitude is conserved since $\psi'=e^{i\alpha}\psi$ gives $|\psi'|^2=\psi^*e^{-i\alpha}e^{i\alpha}\psi=|\psi|^2$.

We can construct unitary matrices from either Hermitian or anti-Hermitian matrices by using the {\bf matrix exponential}. This formally interprets the exponential series in terms of a real or complex valued $n\times n$ matrix $M$ as
\[ \exp(M)\equiv\sum_{n=0}^\infty \frac{M^n}{n!}=I+M+\frac{1}{2}M^2+\frac{1}{6}M^3+\ldots. \]
This series can be shown to always converge, so the series is well defined. Since this is a series of non-commuting objects we again have to be careful with using properties of the exponential from ordinary arithmetic. The following useful properties of the matrix exponential can be proven:
\begin{itemize}
\item[i)] $\exp(A^T)=\exp(A)^T$
\item[ii)] $\exp(A^*)=\exp(A)^*$
\item[iii)] If $B$ is an invertible matrix then $\exp(BAB^{-1})=B\exp(A)B^{-1}$
\item[iv)] If $[A,B]=0$ then $\exp(A)\exp(B)=\exp(A+B)$
\item[v)] $\det e^A = e^{\Tr A}$. 
\end{itemize}
% TODO: Discuss the use of Jordan canonical form in calculations here? Uses iii) to rewrite on Jordan canonical form $J=MAM^{-1}$ where $e^J$ is easy to evaluate because of the Jordan canonical form properties.
% Used in proofs?

Given a Hermitian matrix $M$, $M^\dagger=M$, the matrix $U=e^{iM}$ is then automatically unitary since $U^\dagger=e^{-iM^\dagger}=e^{-iM}$ so that $U^\dagger U=e^{-iM}e^{iM}=I$, where the last equality is due to $M$ commuting with itself. We shall later in this chapter show that in fact all unitary matrices can be written in terms of Hermitian matrices like this. This is the physics construction that we will mostly use in these notes because of our quantum fondness for Hermitian operators. For an anti-Hermitian matrix $M$, $M^\dagger=-M$, we could instead use that $U=e^M$ is automatically unitary since $U^\dagger=e^{-M}$ so that $U^\dagger U=e^{-M}e^{M}=I$. This is the mathematical construction found in a lot of mathematical literature.


\subsubsection{$SU(2)$}
A general member $S$ of $SU(2)$ can be written in terms of two complex parameters $\alpha,\beta\in\mathbb{C}$ as
\begin{equation}
S(\alpha,\beta)=\left[
\begin{matrix} \alpha & -\beta^* \\ \beta & \alpha^* \end{matrix}
\right],
\label{eq:SU2_parameterisation}
\end{equation}
with the additional constraint $|\alpha|^2+|\beta|^2=1$, leaving three free parameters as expected. 

There is a deep connection between the $SU(2)$ group and spin because the Hermitian {\bf Pauli matrices} 
\begin{equation}
\sigma^1 =\left[\begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix}\right], 
\quad \sigma^2 =\left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right], 
\quad \sigma^3 =\left[\begin{matrix} 1 & 0 \\ 0 & -1 \end{matrix}\right],
\label{eq:pauli_matrices}
\end{equation}
that make up the spin-operators $S^i=\frac{\hbar}{2}\sigma^i$, are members of $SU(2)$, and indeed, as we shall see, they can be used to generate (almost) any member of $SU(2)$ by the exponential construction $U=e^{i\alpha_i\sigma^i}$, where $\alpha_i\in\mathbb R$ are three real parameters.\footnote{Make a note of the notation here, the position of the indices is used to prepare the ground for using these in a four-vector $\sigma^\mu=(\sigma^0,\boldsymbol{\sigma})$ together with the identity matrix 
\[ \sigma^0=I_2=\left[ \begin{matrix}1 &0  \\ 0 & 1  \end{matrix} \right].\] }


%%%
\subsection{Orthogonal and special orthogonal groups}
If we restrict the unitary matrices to be real, we get the orthogonal groups.
\df{The {\bf orthogonal group} $O(n)$  is the group of real  $n\times n$  orthogonal matrices $O$, {\it i.e.}\ matrices where $O^TO=I$. If we additionally require that $\det(O) = 1$ the matrices form the {\bf special orthogonal group} $SO(n)$.}
It follows from the definition of the orthogonal group that the determinant of the members is either 1 or $-1$, thus the special orthogonal group is simply one half of the members. For $\mathbf{x} \in \mathbb{R}^n$ the orthogonal group has the same property as the unitary group of leaving the length of vectors invariant. 

Matrices in the $O(n)$ and $SO(n)$ groups have $n(n-1)/2$ independent parameters since an $n\times n$ matrix with real entries has $n^2$ elements, and there are $n(n+1)/2$ equations to be satisfied by the orthogonality condition.\footnote{Only the upper triangular part of $O^TO$ has independent equations since $O^TO$ is a symmetric matrix, $(O^TO)^T=O^T(O^T)^T=O^TO$. } 

The special orthogonal groups $SO(2)$ and $SO(3)$ are much used because their elements represent rotations in two and three dimensions, respectively, while $SO(n)$ extends this to higher dimensions and represents the symmetries of a sphere in $n$ dimensions. To see this we can start from the fact that rotations, by definition, conserve angles and distances (and orientation). This means that the original set of orthogonal axis -- or orthogonal basis vectors if you wish -- must transform into another orthogonal set of axis under the rotation. The matrix performing the rotation must then be orthogonal, and thus  the collection of rotations must be $O(n)$. If we additionally require that orientation is preserved, this removes the matrices with negative determinant, leaving the $SO(n)$ group.


\subsubsection{$SO(2)$}
Given that $SO(2)$ has only one parameter, we can write a general group member $R$ as parameterised by $\theta$
\begin{equation}
R(\theta)=\left[\begin{matrix} \cos\theta &  -\sin\theta \\ \sin\theta&  \cos\theta \end{matrix}\right].
\label{eq:SO2_parameterisation}
\end{equation}
As expected, we recognise this as the matrix of rotations of an angle $\theta$ around a point in the plane, and it represents the (orientation preserving) symmetries of a circle. Some tinkering with this representation will show that $SO(2)$ is in fact an abelian group, despite the matrix definition. From a physical viewpoint this should be expected: the order of rotations in the plane should not matter.

It is interesting to observe that the elements of $SO(2)$ rotate points in the plane, while the elements of $U(1)$ rotate complex numbers, which can be represented by points in the plane. Indeed, a one-to-one correspondence can be found between the members of the two groups so that the groups are indeed the same, or, as we say, isomorphic, $SO(2)\cong U(1)$.

\subsubsection{$SO(3)$}
This group has three free parameters. Already at this point writing down the explicit form of a general group member is not very enlightening. There are also a number of different conventions in use, so proper care is advised when using results from the literature. In terms of a general rotation in three dimensions this can either be viewed as rotation angles around three fixed axis, or as the fixing of a rotation axis by two angles, with a third rotation angle around that axis. 

One particular explicit form, where the angles correspond to the three {\bf Euler angles} of rotation in three dimensions, $\alpha$, $\beta$ and $\gamma$, is
\[
R(\alpha,\beta,\gamma)=\left[\begin{matrix} 
\cos\alpha\cos\beta\cos\gamma-\sin\alpha\sin\gamma & -\cos\alpha\cos\beta\sin\gamma-\sin\alpha\cos\gamma & \cos\alpha\sin\beta  \\ 
\sin\alpha\cos\beta\cos\gamma+\cos\alpha\sin\gamma & -\sin\alpha\cos\beta\sin\gamma+\cos\alpha\cos\gamma & \sin\alpha\sin\beta  \\ 
-\sin\beta\cos\gamma & \sin\beta\sin\gamma  & \cos\beta 
\end{matrix}\right],
\]
where $0\le\alpha,\gamma< 2\pi$ and $0\le\beta\le\pi$. These rotations do not commute, so the group is non-abelian.

We have seen above that $SU(2)$ has three free parameters, just the same as $SO(3)$. You may at this point guess that $SO(3)$ is isomorphic to $SU(2)$. That would be a very good guess, however, it would also be wrong. We will return to this later, but this is one of those things in mathematics that turn out to be disappointingly only almost true.


%%%
\subsection{Symplectic and compact symplectic groups$^*$}

\df{The {\bf symplectic group} $Sp(2n)$  is the group of $2n\times 2n$ symplectic matrices $M$, {\it i.e.}\ matrices where $M^T\Omega M=\Omega$, with
\[ \Omega= \left[ \begin{matrix} 0 & I_n \\ -I_n & 0 \end{matrix} \right].\]
The  {\bf compact symplectic group} $Sp(n)$ is the  intersection of the symplectic group $Sp(2n)$ and the unitary group $U(2n)$, {\it i.e.}\ 
\[ Sp(n)\equiv Sp(2n)\cap U(2n), \]
so that its matrices are members of both groups.\footnote{The $n$ as apposed to $2n$ here is intentional, the reason is that the compact symplectic group can be described in terms of the unitary group $U(n,\mathbb H)$ of $n\times n$ matrices over the quaternions (see Sec.~\ref{sec:normed_division_algebras}).} 
}

The choice here of $\Omega$ can be generalised to any nonsingular skew-symmetric matrix, {\it i.e.} an invertible matrix $\Omega$ where $\Omega^T=-\Omega$, however, this does not change the group structure.
Note that we can easily show $\det\Omega=1$ and $\Omega^{-1}=\Omega^T=-\Omega$ for our choice of $\Omega$. This in turn implies that the symplectic matrices have $\det M=\pm1$, but this can be strengthened to show that indeed $\det M=1$.

The compact symplectic group has interesting applications in classical mechanics. If a system of $n$ particles has generalised coordinates $q_i$ and momenta $p_i$ we can stack the coordinates in the vector $\boldsymbol \eta=(q_1,\ldots,q_n,p_1,\ldots,p_n)^T$. It can then be shown that any symplectic  matrix $M$ acting on $\boldsymbol\eta$ is a {\bf canonical transformation}, meaning it is a transformation that leaves Hamiltons equations unchanged, so the symplectic groups are the symmetry groups of Hamilton's equations.

In quantum mechanics we can write the canonical commutation relations $[\hat q_i, \hat p_j]=i\hbar\delta_{ij}$ as
\[ [\hat{\boldsymbol \eta}, \hat{\boldsymbol \eta}^T]=i\hbar\Omega. \]


%%%%%%%%%%%%%%
\section{Group properties}
\label{sec:group_properties}
%%%%%%%%%%%%%%

%%%
\subsection{Subgroups}
We now extend our vocabulary for groups by defining the {\bf subgroup} of a group $G$.
\df{A subset $H \subset G$ is a {\bf subgroup} if and only if:\footnote{An alternative, equivalent, and more compact way of writing these two requirements is the single requirement $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$. This is often utilised in proofs.}
\begin{enumerate}[i)]
\item $h_i \circ h_j \in H$  for all  $h_i,h_j\in H$, \hfill (closure)
\item $h_i^{-1} \in H$ for all $h_i\in H$. \hfill (inverse)
\end{enumerate}
$H$ is a {\bf proper} subgroup if and only if $H\neq G$ and $H \neq \{e\}$. }
We have already seen some examples of subgroups: the $SU(n)$ groups are subgroups of $U(n)$, and the $SO(n)$ groups are subgroups of the $O(n)$ groups. This can easily be shown using the properties of determinants.

There is a very important class of subgroup called the {\bf normal} subgroup. The importance will become clear in a moment.
\df{A subgroup $H$ is a {\bf normal} (invariant) subgroup, if and only if the {\bf conjugation} of any element $h\in H$ by any $g \in G$ is in $H$,\footnote{Another, pretty, but slightly abusive, way of writing the definition of a normal group is to say that $gHg^{-1}=H$. This implies (correctly), that the image if $H$ under the conjugation operation is guaranteed to be the whole of $H$.}
meaning
\[ ghg^{-1} \in H\text{ for all } h \in H.\]
A {\bf simple} group $G$ has no proper normal subgroup. A {\bf semi-simple} group $G$ has no proper abelian normal subgroup.}
We can for example show that for $n>1$, $SU(n)$ is a normal subgroup of $U(n)$, see Ex.~\ref{ex:SUn_normal}. 

%%%
\subsection{Quotient groups}
The normal subgroup can be seen as a factor in the original group that can be divided out to form a simpler group that only retains the structure that was not in the normal group. To be more precise we need the concept of {\bf cosets}. 
\df{A {\bf left coset} of a subgroup $H\subset G$ with respect to $g\in G$ is the set of members $\{gh | h\in H\}$, and a {\bf right coset} of the subgroup is the set  $\{hg|h\in H\}$. These are sometimes written $gH$ and $Hg$, respectively.}
Despite the appearance of containing many of the same members we can show any two cosets are either disjoint or identical sets.

For normal subgroups $H$ it can be shown that the sets of left and right cosets $gH$ and $Hg$ coincide and form a group. This is called the {\bf quotient}  or {\bf coset group} and denoted $G/H$.\footnote{Sometimes also called the {\bf factor group}. The notation $G/H$ is pronounced ``G mod H'', where ``mod'' is short for modulo.}
This has as its members all the {\it distinct sets} $\{gh|h\in H\}$, that can be generated by a $g\in G$, and has the binary operation $*$ with $\{gh|h\in H\}*\{g'h|h\in H\}=\{ (g\circ g')h| h\in H\}$. To simplify notation this can be written $gH*g'H=(g\circ g') H$. 
 
To make some sense of this, let us briefly discuss an example of a quotient group. We already know that $SU(n)$ is a normal subgroup to $U(n)$. This means that $U(n)/SU(n)$ is a group. What sort of group is this? Notice that two matrices $U_i$ and $U_j$ live in the same coset of $SU(n)$ if and only if $\det(U_i)=\det(U_j)$. In other words, each coset constructed from $SU(n)$ is simply the set of all matrices with a given determinant, which we saw for the $U(n)$ group can be any unit complex number. Observe that this means that many of the cosets are the same, all cosets generated by members in $U(n)$ with the same determinant is the same coset. 
When these cosets act on each other with the group operation of $U(n)/SU(n)$ they form new cosets of matrices with a determinant that is the product of their individual determinants, {\it i.e.}\ with $U,U'\in U(n)$ we have the product of quotient group member $\{US|S\in SU(n)\}*\{U'S| S\in SU(n)\}=\{ UU'S| S\in SU(n)\}$. Thus, the group behaves exactly as $U(1)$, and is in fact isomorphic to it.

%%%
\subsection{Product groups}
Now that we have introduced group division,  we also need to introduce products of groups. 
\df{The {\bf direct product} of groups $G$ and $H$, $G\times H$, is defined as the ordered pairs $(g,h)$ where $g\in G$ and $h\in H$, with component-wise operation $(g_i,h_i)\circ(g_j, h_j) = (g_i\circ g_j , h_i \circ h_j)$. $G\times H$ is then a group and $G$ and $H$ can be shown to be normal subgroups of $G\times H$.}
We should note here that the subgroups are strictly $G\times\{e_H\}$ and $\{e_G\}\times H$, but these are isomorphic to $G$ and $H$.

Because it has an important appearance in this text we also need the definition of the {\bf semi-direct product}.
\df{The {\bf semi-direct product} of groups $G$ and $H$, $G\rtimes H$, where $G$ is also a mapping $G:H\to H$, is defined by the ordered pairs $(g,h)$ where $g\in G$ and $h\in H$, with component-wise operation $(g_i,h_i)\circ(g_j, h_j) = (g_i\circ g_j , h_i \circ g_i(h_j))$. Here $H$ is not a normal subgroup of $G\rtimes H$, but $G$ is.}
Note how the semi-direct product is not symmetric between the factors.

The famous Standard Model gauge group $SU(3) \times SU(2)\times U(1)$ is an example of a direct product. Direct products are ``trivial" structures because there is no ``interaction" between the subgroups, the elements of each group act only on elements of the same group. For the Standard Model this means that the each of the gauge transformations can act independently on the states unaffected by any other gauge transformation. This is not true for semi-direct products. 

% TODO: More examples here. Use Euclidean group

%%%
\subsection{Isomorphic groups}
We have already talked about how two groups are the same if they have a one-to-one correspondence between their members and the same results for the multiplication operation, and we have called this an isometry. Let us now try to put this notion of when two groups are the same into a more formal language.
\df{Two groups $G$ and $H$ are {\bf homomorphic} if there exists a map between the elements of the groups $\rho:G\to H$, such that for all $g,g'\in G$,  $\rho(g\circ  g')=\rho(g)\circ \rho(g')$.}
For homomorphic groups we say that the mapping conserves the structure of the group, or in other words, all the rules for the group operation/multiplication. This leads to our notion of group equality, namely {\bf isomorphic} groups:
\df{Two groups $G$ and $H$ are {\bf isomomorphic}, written $G\cong H$, if they are homomorphic and the relevant mapping is one-to-one (injective) and onto (surjective).}
The one-to-one and onto (hitting all elements of $H$) mapping ensures that there is a one-to-one correspondence between the elements of the two groups, so that isomorphic groups effectively contain both the same members and have the same multiplication operation.

For matrix groups, a good way of checking the plausibility of isomorphism is to count the number of free parameters. The difference of the parameters for the two factors should be equal to the number of parameters for the quotient group. In our example $U(n)/SU(n)$, $U(n)$ has $n^2$, while $SU(n)$ has $n^2-1$, and this gives $n^2-(n^2-1)=1$ parameters for the quotient group, which matches the one parameter of $U(1)$. 

At the end of this subsection, let us make a warning: despite the enticing notation it is {\it not} in general the case that if $H$ is a normal subgroup to $G$, then $G\cong G/H\times H$.


%%%%%%%%%%%%%
\section{Representations}
\label{sec:rep}
%%%%%%%%%%%%%
In some contrast to the treatment in most introductory group theory texts in mathematics, physicists are mostly interested in the properties of groups $G$ where the elements of $G$ {\it act} to transform some elements of a vector space $v\in V$, $g(v) = v' \in V$. This part of group theory is called {\bf representation theory}. Here, the members of $V$ can for example be the state of a system, say a wave-function in quantum mechanics or a field in quantum field theory. 
To be useful in physics, we would like that the result of the group operation $g_i \circ g_j$ acts as $(g_i\circ g_j)(v) = g_i(g_j(v))$ and the group identity acts as $e(v) = v$.


%%%
\subsection{Definition}
We begin with the abstract definition of a representation that ensures these properties. 
\df{A {\bf representation} of a group $G$ on a vector space $V$ over the field $K$ is a map $\rho : G \to GL(V,K)$, where $GL(V,K)$ is the {\bf general linear group of the vector space} $V$,\footnote{The general linear group of the abstract vector space $V$ is the set of all one-to-one and onto linear transformations $V\to V$, together with functional composition as group operation. For finite dimensional spaces this group is isomorphic to the general linear group $GL(n,K)$ that we introduced earlier.} such that for all $ g_i,g_j\in G$,
\begin{equation}
\rho(g_i\circ g_j) = \rho(g_i)\rho(g_j). \text{    (homomorphism)}\nonumber
\end{equation}
If this map is also isomorphic, we say that the representation is {\bf faithful}.
}
Here $V$ is called the {\bf representation space} and the dimension $n$ of $V$ is called the dimension of the representation. While somewhat confusing, it is common to talk about $V$ as the representation itself, even though it is really the space on which the representation acts. 

To see that this fulfils our wanted properties, notice that $\rho(g)=\rho(e\circ g)=\rho(e)\rho(g)$, which means that $\rho(e)= \mathbb{1}$ must be the identity transformation on $V$. The concatenation property follows directly from the homomorphism.

Writing this definition in terms of a generic vector space $V$ and field $K$ may be a bit obfuscating, but we need to keep in mind that our group elements can be acting on members of abstract vector space, {\it e.g.}\ spaces where the members of the space are functions, such as in the earlier example of the translation group acting on functions. In the case where $V$ is finite dimensional it is common to choose a concrete basis for $V$ and identify $GL(V,K)$ with $GL(n, K)$, the group of $n\times n$ invertible matrices with elements from the field $K$. So, in many cases, the representations we are interested in are matrices acting on coordinate space vectors over the fields $\mathbb R$ or $\mathbb C$, {\it i.e.} vectors in $\mathbb R^n$ or $\mathbb C^n$.

From a physics point of view, the underlying point here is that (the members of) our groups will be used on quantum mechanical states, or fields in field theory, which can be just complex numbers or functions, or multi-component vectors of such. They are thus members of a vector space, and the definition of representations allows the transformation properties of the group to be written in terms of matrices for finite dimensional representation spaces. Furthermore, the mapping from the group, or, if you like, the concrete way of writing the abstract group elements, must be homomorphic (structure preserving), meaning that if we can write a group element as the product of two others, the matrix for that element must be the product of the two matrices for the individual group elements it can be written in terms of.

%% TODO: Separate section here on unitary representations? Relate to compact. Existence of finite-dimensional
%% TODO: Discuss the difference between active and passive view of transformation, e.g. Williams Sec. 1.2
% TODO: Linear representations as a special category

In physics an important category of representations are the {\bf unitary representations}, which are representations $\rho$ of a group $G$ on a complex Hilbert space $V$ where $\rho(g)$ is a unitary operator for every $g\in G$, {\it i.e.}\ an operator that preserves the length of vectors in $V$. This generalises the unitary (matrix) group, which are examples of unitary representations, beyond finite dimensional vector spaces.


%%%
\subsection{Representation examples}
You may by now have realised that the matrix groups defined in Sec.~\ref{sec:matrix_groups} have the property that they are defined in terms of one of their representations. These are called the {\bf fundamental} or {\bf defining  representations}. However, we will also have use for other representations, {\it e.g.}\ the {\bf adjoint representation} which we will introduce later.

Let us now take a few examples that connect to our definition and to relevant physics. We saw earlier that for $U(1)$ the group members can be written in the fundamental representation as the complex numbers on the unit circle $e^{i \theta}$, which can be used as phase transformations on wavefunctions $\psi(x)$. This can be viewed as the action on a one-dimensional space over $\mathbb C$ where the basis vector is the wavefunction.

For $SU(2)$ we saw that we needed three real numbers to parametrise the group elements. It then seems reasonable that we should be able to write a general matrix in $SU(2)$ in terms of the linear combination of three unitary ``basis'' matrices, for example the {\bf Pauli matrices} in (\ref{eq:pauli_matrices}). Since the three Pauli matrices are linearly independent, the sum $\alpha_i\sigma^i$, $\alpha_i\in\mathbb R$, should (hopefully) in some sense span all of $SU(2)$.\footnote{We will later see to what extent this is true, but we must emphasise here that $SU(2)$ is not a vector space, so it is not spanning in the vector space sense.} As it turns out, the group elements of $SU(2)$ in the fundamental representation can indeed written as the exponentials  $e^{i \alpha_i \sigma^i}$. We will return to why we use an exponential in Sec.~\ref{sec:lie_algebras}. In physics the fundamental representation of $SU(2)$ is often denoted {\bf 2}, following the pattern that the representations are written up with the dimension of their matrices in boldface. In the Standard Model the fundamental representation is used on the vector space of doublets of fermion fields, {\it e.g.}\ the electron--neutrino doublet $\psi = (\nu_e, e)^T$ that form a two-dimensional vector space, as the $SU(2)_L$ gauge transformation. 

However, we can construct many more representations than the fundamental from a single group such as $SU(2)$. Using the three free parameters in $SU(2)$ it turns out that we can also represent the elements in the group in terms of three  $3\times3$-matrices that act on vectors in a three-dimensional space. In place of the Pauli matrices we can use the three (anti-Hermitian) matrices
\begin{equation}
L^1 =\left[\begin{matrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0  \end{matrix}\right], 
\quad L^2 =\left[\begin{matrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0  \end{matrix}\right], 
\quad L^3 =\left[\begin{matrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0  \end{matrix}\right].
\label{eq:SO3_generators}
\end{equation}
You may recognise these as related to the angular momentum operators in quantum mechanics in the Heisenberg picture. As we will see later, these matrices will form the basis for creating the three-dimensional adjoint representation of $SU(2)$, named {\bf 3}. In the Standard Model this is the representation used for operations on the gauge fields that transform under $SU(2)_L$, which are represented as three-component vectors. The central point here is that the group structure is the same (isometric), even if the objects in the representation are different.

At this point you may be worried about the translation group $T(1)$ that we saw earlier and its extensions to higher dimensions. There we wrote down the elements using differential operators $T_a=e^{a\frac{\partial}{\partial x}}$. This does not look a lot like matrices. However, here the abstract vector space that is acted on by the group elements is made up of smooth (infinitely differentiable) functions, and in our generic definition of a representation above there is no need for the general linear group of such a space to be written in terms of matrices. Unless we are very strict with what functions we allow, this is typically an infinite dimensional space. It is a bit tricky to prove that our differential operators $T_a$ form a representation, however, they do, and these kinds of group representations are called {\bf differential representations}.

The existence of multiple representations for the same group necessitates a definition of when representations are actually equivalent, or isomorphic. This should not be confused with whether groups are isomorphic, but removes differences in representations that are simply due to a change in basis for the vector space the group is acting on, or trivial changes in the dimension of the vector space.\footnote{Imagine, for example, that you create a representation for $U(1)$ that consists of diagonal $2\times2$-matrices with the unit complex numbers repeated twice on the diagonal. This is not essentially different from the one-dimensional representation, and should not be considered as such.} 
\df{Two representations $\rho$ and $\rho'$ of $G$ on $V$ and $V'$ are {\bf equivalent} if and only if there exists a map $A:V\to V'$, that is one-to-one and onto, such that for all $g \in G$, $A\rho(g)A^{-1} = \rho'(g)$.}
We again give the definition for an abstract representation space. For a finite-dimensional space with a given basis, $A$ and $\rho(g)$ are simply matrices.


%%%
\subsection{Irreducible representations}
\label{sec:irreps}
The building blocks of representations are so-called {\bf irreducible representations}, also called {\bf irreps}. These are the essential ingredients in representation theory, and are defined as follows: 
\df{An {\bf irreducible representation} $\rho$ of a group $G$ is a representation where there is {\it no} proper subspace $W \subset V$ that is closed under the group, {\it i.e.}\ there is no $W\subset V$ such that for all $w \in W$, and all $g \in G$ we have $\rho(g)w \in W$.\footnote{In other words, we can not split the matrix representation of $G$ in two parts that do not ``mix".}}

Let us take an example to try to clear up what a {\it reducible} representation means in contrast to an irreducible. Assume the representation $\rho(g)$ for $g \in G$ acts on a vector space $V$ as matrices. If these matrices $\rho(g)$ can all be decomposed into $\rho_1(g)$, $\rho_2(g)$ and $\rho_{12}(g)$ such that for $\mathbf v =(\mathbf v_1, \mathbf v_2)\in V$ 
\[\rho(g) \mathbf v = \begin{bmatrix}\rho_1(g) & \rho_{12}(g) \\ 0 & \rho_2(g)\end{bmatrix} \begin{bmatrix}\mathbf v_1 \\ \mathbf v_2 \end{bmatrix}  ,\]
then $\rho$ is {\bf reducible}. The subspace $W$ of $V$ spanned by $\mathbf v_1$ violates the irreducibility condition above.

If  we also have $\rho_{12}(g)=0$ in our example we say that the representation is {\bf completely reducible}. It can be shown that in most cases a reducible representation is also completely reducible. In fact, representations for which this is not true tend to be mathematical curiosities. For example, if the representation is unitary and the vector space is a Hilbert space (states in quantum mechanics), we can prove that the representation is always completely reducible.  As a result, there is a tendency in physics to use the term ``reducible" where we should maybe use the term ``completely reducible". 

In the case of a completely reducible representation we can split the vector space $V$ into a direct sum of two vector spaces $V=V_1\oplus V_2$, where $\mathbf v_1\in V_1$ and $\mathbf v_2\in V_2$, and define a representation of $G$ on each of them using $\rho_1$ and $\rho_2$, which in turn could either be reduced more, or would themselves be irreducible. This process can then be continued until the representation has been broken down into only irreducible components.

We end this section with an important theorem that helps us decide whether a representation is irreducible, and ultimately gives a property identifying the representation. As with many important theorems, it started its life as a lemma.
\theo{{\bf (Schur's Lemma~\cite{Schur})}\\ If we have an irreducible representation $\rho$ of a group $G$ on a vector space $V$ and  a linear operator $A:V\to V$ that commutes with $\rho(g)$ for all $g \in G$, then  $A$ is proportional to the identity map, $A=\lambda \mathbb{1}$. The constant of proportionality $\lambda$ can be used to label the representation.}

%\bigskip
%{\it Proof:} First we will see that if $A$ is not trivial, meaning $A=0$, then $\det A\neq 0$.
%
%
%Let $V$ be the carrier space of $\rho(g)$. This means that any $\rho(g)$ acting on any element of $V$ produces another element of $V$. We introduce this in an obvious shorthand notation, $M V = V$. Let us begin by assuming that $A$ operating on elements of $V$ always produce an element in the subspace $V'\subset V$, $AV = V'$. This indirectly implies that $\det A = 0$ since otherwise
%
% If $A$ commutes with $M$, $[M,A]=0$, then 
%\[(MA)V = (AM)V,\]
%so
%\[MV' =AV =V'.\]
%Thus $M$ operating on $V'$ produces another vector in $V'$, so $V'$ must be an invariant subspace of $V$, which is inconsistent with our the assumption that $M$ is irreducible. This contradiction could be avoided if either $A$ is trivial,  $A=0$, or if $\det(A) \ne 0$, because then $V$ would be identical to $V'$. 
%
%Since we must have $\det A\neq 0$, we can write $A=B+\lambda I$ where $\det B=0$
%In this case we could still write $A=B+\lambda I$, where $\det(B)=0$ and $\det(A)=\lambda$. Then $[M,A]=0$ implies $[M,B] = 0$; and the only remaining way to avoid the contradiction is to admit that $B = 0$, and consequently $A$ is a multiple of the unit matrix.
%

Schur's lemma as stated here has again been formulated in the language of general vector spaces. In terms of matrix representations it says that any matrix $A$ that commutes with all the matrices in an irreducible representation must be equal to a constant times the identity matrix, {\it i.e.}\ $A=\lambda I$.

The constant of proportionality $\lambda$ is unique to the representation up to a common normalisation constant that can be incorporated into the linear operator/matrix (a constant multiple of $A$ would still have the same commutation properties). For us, these linear operators $A$ will generally turn out to have a very specific physical interpretation, so their natural normalisation will be clear. As a result the constants can be used to index the different irreducible representations. The proof of this property is somewhat beyond the scope of these notes.



%%%%%%%%%%
\section{Lie groups}
\label{sec:Lie_groups}
%%%%%%%%%%


%%%
\subsection{Definitions}
\label{sec:Lie_group_definitions}
In physics we are particularly interested in a special type of continuous groups that we can parametrise, the {\bf Lie groups}, which are the basic tools we use to describe continuous symmetries. The abstract mathematical notion of continuity comes from topology, along with a notion of open sets and proximity, so continuous groups are also called {\bf topological groups}. For topological groups the group multiplication and inverse need to be continuous maps. We further say that a topological group is a {\bf compact group} if its topology is compact, meaning that it has no punctures or missing endpoints, {\it i.e.}\ it includes all limiting values of the group members. In practise the topology of our groups will be subspaces of Euclidean space $E^n$, in which case the space is compact if and only if it is closed and bounded. We will return to some examples of this later.

A Lie group extends the continuity of topological groups by requiring the multiplication and inverse maps to be {\it smooth} (infinitely differentiable $C^\infty$).
In order to define Lie groups we will need to use the technical term {\bf (smooth) manifold}, meaning a mathematical object (formally a topological space) that locally\footnote{This insistence on local means that the parameterisation is not necessarily the same for the whole group.} can be parametrised as a function of  $\mathbb{R}^n$ or $\mathbb{C}^n$. We will thus describe a Lie group $G$ as a manifold in terms of a parameterisation of the members $g(\mathbf a)\in G$, where $\mathbf a\in \mathbb{R}^n$ (or $\mathbb{C}^n$). Additionally, in order to describe continuous symmetries these parameterisations need to be smooth.
\df{A {\bf Lie group} $G$ is a finite-dimensional {\bf smooth manifold} where group multiplication and inversion are smooth functions, meaning that given elements $g(\mathbf{a}), g(\mathbf{a}') \in G$, $g(\mathbf{a})\circ g(\mathbf{a}') = g(\mathbf{b})$ where $\mathbf{b}(\mathbf{a}, \mathbf{a}')$ is a smooth function of $\mathbf a$ and $\mathbf a'$, and $g^{-1}(\mathbf{a}) = g(\mathbf{b})$ where $\mathbf{b}(\mathbf{a})$ is a smooth function of $\mathbf a$.}
The dimension of a Lie group is the dimension $n$ (or $2n$) of the manifold.

From our earlier example we immediately see that the translation group $T(1)$, given the parameterisation of the elements $g(a) = e^{a\frac{\partial}{\partial x}}$, is a Lie group since $g(a)g(a') =g(b)= g(a+a')$ and $b=a+a'$ is an analytic function of $a$ and $a'$, and for the inverse $g^{-1}(a)=g(b)=g(-a)$ where $b=-a$ is a smooth function of $a$. Since $a\in\mathbb R$ is unbounded the group members are also unbounded, and thus the group is not compact.

All the matrix groups are also Lie groups. While their dimension and parameterisation is clear, for example a member of $GL(n,\mathbb R)$ has $n^2$ parameters by its elements, to prove that they are in fact Lie groups is tricky because of the requirement on the non-zero determinant. The proof is done by showing it for the general linear group, and then using the {\bf closed subgroup theorem}, saying that any closed (in the topological sense) subgroup of a Lie group is a Lie group, which applies to the other matrix groups. 

We saw several examples of parameterisations of matrix groups earlier, for example $U(1)$ could be parameterised by a single parameter $\theta\in [0,2\pi]$ by writing an element $U\in U(1)$ as the complex number $U=e^{i\theta}$. We can then identify the group topologically with a subspace of $E^2$, and since $|U|=1$ this group is bounded and closed, and thus a compact group. Similar arguments can be made for the compactness of all the matrix groups $U(n)$, $SU(n)$, $O(n)$, $SO(n)$, $Sp(2n)$, and $Sp(n)$, however, $GL(n)$ and $SL(n)$ are not compact because their elements are not bounded.


%%%
\subsection{Generators}
\label{sec:generators}

The situation we are usually in in physics is that of a $d$-dimensional Lie group $G$ acting on a vector space $V$ through a representation. This representation vector space  may be finite dimensional, in which case its members $\mathbf x\in V$ are written as vectors in some specific basis, or it may be infinite dimensional and consist of functions that in turn have a domain $\mathbb R^n$ or $\mathbb C^n$.

For finite-dimensional representations we can locally write the map of the representation $G\times V \to V$ for $\mathbf{x} \in V$ in terms of an explicit function $\mathbf f$ called the {\bf composition function}. We have $x_i \to x_i' = f_i(\mathbf x, \mathbf a)$, $i=1, \ldots,n$, where the composition function $f_i$ is analytic\footnote{Analytic means infinitely differentiable and in possession of a  convergent Taylor expansion. As a result analytic functions (on $\mathbb{R}$) are smooth, but the reverse does not hold.} in $x_i$ and $a_j$, $j=1,\ldots,d$. Additionally $f_i$ should have an inverse. Note here that the dimensions of $\mathbf x$ and $\mathbf a$ are different, the first is given by the dimensions of the space $V$, $n$, while the second is given by the dimensions $d$ of the Lie group.

By the analyticity of the explicit function $f$ we can always construct the parametrisation so that the zero parameter corresponds to the identity element of the group, $g(0) = e$, which means that  $f_i(\mathbf x, 0)=x_i$.\footnote{If this was not true for our parameterisation we could Taylor expand $\mathbf f$ around the parameter giving the identity element and then redefine the parameterisation by a linear shift.} By an infinitesimal change $d\mathbf a$ of the  parameters we then get the following Taylor expansion\footnote{The fact that $f_i$ is analytic means that this Taylor expansion must converge in some radius around $f_i(\mathbf x,0)$.}
\begin{eqnarray*}
x'_i &=& x_i + dx_i = f_i(\mathbf x, d\mathbf a)\\
&=& f_i(\mathbf x, 0) + \left.\frac{\partial f_i}{\partial a_j}da_j\right|_{\mathbf a =0} +\ldots\\
&=& x_i +\left.da_j \frac{\partial f_i}{\partial a_j}\right|_{\mathbf a =0}.
\end{eqnarray*}
This is the result of the transformation by the member of the group that in the parameterisation sits $d\mathbf a$ from the identity. For a group where the matrix $A$ of the representation is parametrised as $A(\mathbf a)$ the resulting change in the vector space is
\[
dx_i=\left.da_j\frac{\partial f_i}{\partial a_j}\right|_{\mathbf a =0}=da_j\left[\frac{\partial A(\mathbf a)}{\partial a_j}\mathbf{x}\right]_{i,\mathbf a=0}=[ida_jX^j\mathbf{x}]_i,
\]
where we have defined 
\[
iX^j\equiv \left. \frac{\partial A(\mathbf a)}{\partial a_j}\right|_{\mathbf a=0} ,
\]
as the $d$ matrix {\bf generators} $X^j$ of the Lie group.

If we now instead let $F(\mathbf x)$ be a function taken from the (infinite dimensional) vector space $V$ that we are interested in, mapping to either the real $\mathbb{R}$ or complex numbers $\mathbb{C}$, where $\mathbf x$ is now in the $n$-dimensional domain of the function, and letting $\mathbf f$ be the transformation of the domain effected by the group elements, then the group transformation defined by $d\mathbf a$ near the identity changes $F$  by
\begin{eqnarray*}
dF &=& \frac{\partial F}{\partial x_i}dx_i = \frac{\partial F}{\partial x_i}\left.\frac{\partial f_i}{\partial a_j}\right|_{\mathbf a=0}da_j\\
&\equiv& i da_j X^j F,
\end{eqnarray*}
where the operators defined by 
\begin{equation}
iX^j \equiv\left.\frac{\partial f_i}{\partial a_j} \right|_{\mathbf a=0}\frac{\partial }{\partial x_i},
\end{equation}
are called the $d$ differential {\bf generators} of the Lie group. We see here that in both cases the number of generators is the same as the dimension of the manifold, which is also the number of free parameters in the parameterisation of the group.

It is these generators $X$ that then define the effect of the Lie group members in a given representation (near the zero parameter), while the $d$ numbers $a_j$ are mere parameters. We say that the generators determine the local structure of the group. It turns out that this linearisation, often called the {\bf tangent space}, of a group using an infinitesimal change near the origin is sufficient to recover the whole group locally. Note that the generators are not unique, they do depend on the parameterisation of the group, but as we shall see later some of their properties are independent of this.

For the translation group $T(1)$ the action of the group on the domain $\mathbb{R}^1$ of the functions in the representation space, is $x'=f(x,a) = x+a$. The resulting (single) generator is
\[ iX^1=\frac{\partial f}{\partial a} \frac{\partial}{\partial x}= \frac{\partial}{\partial x}. \]
Notice here how we represented a generic element of $T(1)$ by the parameter and the generator as
\[ T_a=e^{ia_j X^j}=e^{a \frac{\partial}{\partial x}}. \]
This is of course no coincidence, and shows the way to a general recipe that we will use.\footnote{While we are in the business of {\it deja vu}, notice also how the generator for translations is $X=-i \frac{\partial}{\partial x}$, which can be compared with the quantum mechanical momentum operator $\hat p=-i\hbar \frac{\partial}{\partial x}$, keeping in mind that the conservation of momentum through Noether's theorem is intimately linked to the invariance of our models under translation. This is a point we will return to later.}

As another example of the above we can now go in the opposite direction and look at the two-parameter coordinate transformation {\it defined} by
\[x' = f(x) = a_1x + a_2,\]
which gives the generators
\[iX^1 = \frac{\partial f}{\partial a_1} \frac{\partial}{\partial x} = x\frac{\partial}{\partial x},\]
which is the generator for {\bf dilation} (scale change) in one dimension $D(1)$, and
\[iX^2 =  \frac{\partial f}{\partial a_2} \frac{\partial}{\partial x} =\frac{\partial}{\partial x},\]
which is again the generator for the translation $T(1)$. 
Notice that we can show the following relationship for these two generators: $[X^1, X^2] \equiv X^1X^2-X^2X^1= iX^2$. This can be seen to hold independently of the parameterisation of the group, something we will generalise later.

As we have seen, the group $SU(2)$ has three free parameters $a_i$, so it must have three generators $X^i$. We can now show that the generators for $SU(2)$ in the two-dimensional representation are indeed proportional to the Pauli matrices in (\ref{eq:pauli_matrices}) as intimated earlier, namely $X^i=\frac{1}{2}\sigma^i$, see Ex.~\ref{ex:SU2_generators}. By multiplying out we can also show the following commutation relationships between the generators (Pauli matrices):
\begin{equation}
[X_i,X_j]=i\epsilon_{ijk}X^k.
\label{eq:su2_algebra}
\end{equation}
These commutators should look familiar to us as they have the same structure as the commutators for the spin $S_i$ operators in quantum mechanics.\footnote{This would be $[S_i,S_j]=i\hbar\epsilon_{ijk}S^k$.} 

For matrix groups there is an alternative method to the generating functions for finding the generators that sees a lot of use. Since we are looking for infinitesimal deviations from the identity (matrix) $I$, we add an infinitesimal matrix $dM$ (the generator) and look at the properties of $dM$ given the definition of the group. Let us take $SO(2)$ as an example. Here a matrix in the group $O$ must fulfil $O^TO=I$ and $\det O=1$. Writing near the identity $O=I+dM$ we get the requirement $O^TO=I+dM+dM^T=I$ to first order in the infinitesimal. Thus the generator must fulfil $dM=-dM^T$ (it is a real, anti-symmetric matrix). This means that the diagonal of $dM$ must be zero, and in two dimensions, up to a constant, the only matrix that fits the role of the generator is
\[ X=\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right],\]
so that the group element can be written $O=I+d\theta X$, where $d\theta$ is some infinitesimal parameter of the group. To check the determinant we need to consider how we use the generator to recreate the group since the generator is defined through an infinitesimal change. For reasons that will become clearer later we use the exponential $O=\exp(\theta X)$, which to first order in an infinitesimal parameter $d\theta$ recreates the behaviour near the identity we used.
By the properties of matrix exponentiation we  can now check that $\det O=\det \exp(\theta X)=\exp(\theta \Tr X)=\exp(0)=1$. Since $X$ is an anti-Hermitian matrix, and we as physicists are more comfortable as Hermitian, we tend to use instead
\[ X=-i\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right]=\left[\begin{matrix} 0 & -i \\ i & 0 \end{matrix}\right],\]
as the generator, writing the group element as $O=\exp(i\theta X)$.

A similar calculation for $SO(3)$, which has three free parameters, gives the generators $X^i=\frac{1}{2}L^i$, where $L^i$ are the three matrices in Eq~(\ref{eq:SO3_generators}). These generators have the exact same commutator as the $SU(2)$ generators, $[X_i,X_j]=\epsilon_{ijk}X^k$, up to a constant factor, indicating a structural similarity between the groups $SU(2)$ and $SO(3)$. For $SO(3)$ we can further find a corresponding set of differential generators
\begin{equation}
L_x=-i\left(y\frac{\partial}{\partial z}-z\frac{\partial}{\partial y} \right),\quad
L_y=-i\left(z\frac{\partial}{\partial x}-x\frac{\partial}{\partial z} \right),\quad
L_z=-i\left(x\frac{\partial}{\partial y}-y\frac{\partial}{\partial x} \right),
\label{eq:SO3_diff_generators}
\end{equation}
see Ex.~\ref{ex:SO2_diff_generators}.
Up to a constant factor of $\hbar$ you may recognise these operators as the angular momentum operators in quantum mechanics. This is not a coincidence, the angular momentum operators in quantum mechanics are a representation of the generators of the group of rotations in three dimensions. In fact, the possibility of going back and forth between differential representations and matrix representations of the Lie group is the essence of the duality between the Schr\"odinger and Heisenberg pictures of quantum mechanics.


%%%
\subsection{Structure constants}
\label{sec:structure_constants}

In general, the commutator of the generators of a Lie group satisfy \[[X_i,X_j]=iC_{ij}^{~~k}X_k,\] where  $C_{ij}^{~~k}$ are called the {\bf structure constants} of the group.\footnote{There is an annoying difference in notation here between physics and mathematics, where the $i$ is commonly dropped. This has the same origin as the Hermitian versus anti-Hermitian operator used to create unitary operators discussed earlier.} 
This implies that the generators close under the commutation operation.

We can easily see that these commutators are antisymmetric in the indices $i$ and $j$, 
\[C_{ij}^{~~k} = -C_{ji}^{~~k}.\]
We can also show that there is a {\bf Jacobi identity} among the generators that just directly follows from the properties of the commutator,
\begin{equation}
[X_i, [X_j, X_k]] + [X_j, [X_k, X_i]] + [X_k, [X_i, X_j]] = 0,
\label{eq:Jacobi_generators}
\end{equation}
which in turn leads to a corresponding identity for the structure constants:
\begin{equation}
C_{il}^{~~m}C_{jk}^{~~l} + C_{jl}^{~~m}C_{ki}^{~~l} + C_{kl}^{~~m}C_{ij}^{~~l} = 0.
\end{equation}

Note that just like the generators are basis dependent, so are the structure constants. However, the closure in the commutation relationship is not. For example, we could have chosen the Pauli matrices to be the generators of $SU(2)$. They would then fulfil the commutator relationship
\begin{equation*}
[\sigma_i,\sigma_j]=2i\epsilon_{ijk}\sigma^k, 
\label{eq:pauli_algebra}
\end{equation*}
where the structure constant is $2\epsilon_{ijk}$ instead of $\epsilon_{ijk}$. For the $N$-dimensional unitary groups in physics it is common to use the following normalisation of the structure constants,
\begin{equation}
C_{kl}^{~~i}C_{}^{klj}=N\delta^{ij},
\label{eq:generator_normalisation}
\end{equation}
which coincides with the choices made above for $SU(2)$.


%%%%%%%%%%%
\section{Algebras}
\label{sec:algebras}
%%%%%%%%%%%

To further study the structure of groups we begin by defining an {\bf algebra}. An algebra extends the familiar structure of vector spaces by adding a multiplication operation for the vectors which gives a new vector.
\df{An {\bf algebra} $A$ over a field (say $\mathbb{R}$ or $\mathbb{C}$) is a linear vector space with a binary (multiplication) operation $\circ : A \times A \to A$.}
It is important to remember here that the vector space part of the definition implies that there is field, so for example from $\mathbf x\in A$ and $a\in \mathbb{R}$ (as the field) we can always form new members $a\mathbf x\in A$.

As a very simple example, the vector space $\mathbb{R}^3$ together with the standard cross-product constitutes an algebra since the cross product results in a new vector in  $\mathbb{R}^3$. Even more trivially perhaps is that $\mathbb{R}$ with ordinary multiplication as the binary operation fulfils the algebra requirements (more on this case below).

Connecting to the discussion of Lie groups in Section \ref{sec:Lie_groups}, we see that the generators $X_i$ of a Lie group also span an algebra as basis vectors of a vector space, if we use the commutator as  the multiplication operation between two members. We leave it as an exercise to ponder out how all the criteria for a vector space is fulfilled by the generators, but this is not very difficult.


%%%
\subsection{Normed division algebras$^*$}
\label{sec:normed_division_algebras}
As a slight aside to the main argument of the text, we want to give an interesting example of algebras. We start with a {\bf division algebra}, which informally is an algebra where the binary operation of the algebra also has a meaningful (implicit) concept of division for all members (except division by zero).
\df{An algebra $D$ is a {\bf division algebra} if for any element $x$ in $D$ and any non-zero element $y$ in $D$ there exists precisely one element $z$ in $D$ with $x = y\circ z$ and precisely one element $z'$ in $D$ such that $x = z'\circ y$. In this sense $y$ is a divisor of $x$.}

We see that division algebras have the addition and subtraction properties of ``ordinary numbers'' (reals) since they are vector spaces, and they have the  multiplication (algebra) and division (division algebra) properties of the reals as well. So, in a sense, division algebras are structures close to the reals in terms of properties -- and, of course, the reals are again an example of a division algebra. 

We can now add to the division algebra the notion of a {\bf norm}, or length, of the members $\|x\|$. This is a map from the algebra to the field, $\|\,\|:D\to\mathbb{R}$, so that we can discuss for example convergence of the members as we do for the reals. We have to require here that the norm is homomorphic, meaning that it preserves the structure of the algebra, so that for example the ``product''  of two objects with large norm has a large norm. We then get a {\bf normed division algebra}. 
\df{If there exists a homomorphic norm for the division algebra, {\it i.e.}\ one where $\|x\circ y\|=\|x\|\|y\|$ for all $x,y\in D$, then the division algebra is a {\bf normed division algebra}.}

There is an important theorem by Hurwitz (1923) that demonstrates that only four of these real number ``lookalikes'' exist.
\theo{Hurwitz's theorem. There are only four normed division algebras over the reals (up to isomorphism), the reals themselves $\mathbb{R}$, the complex numbers $\mathbb{C}$,  the quaternions $\mathbb{H}$, and the octonions $\mathbb{O}$.}

In addition it is (relatively speaking) easy to show that $\mathbb{R} \subset\mathbb{C}\subset\mathbb{H}\subset\mathbb{O}$. One perspective on the relationship between these algebras is that the reals is the only ordered normed division algebra, {\it i.e.}, where we can compare uniquely the elements $a>b$. This is not possible for the complex numbers, but they keep the commutative and associate properties of the reals. The quaternions in turn break the commutativity of the complex numbers, while being associative, while the most unruly of the bunch, the octonions, are not even associative.


%%%%%%%%%%%
\section{Lie algebras}
\label{sec:lie_algebras}
%%%%%%%%%%%
We will now turn to the most crucial type of algebras for physics, namely {\bf Lie algebras}. To distinguish these from the more general algebras that we have introduced above, we will use the notation $[\,,\,]$ for the binary operation in Lie algebras. 
\df{A {\bf Lie algebra} $\mathfrak l$  is an algebra where the binary operator $[\,,\,]$, called the {\bf Lie bracket}, has the properties that for $x,y,z \in \mathfrak l$ and $a,b \in \mathbb{R}$ (or $\mathbb{C}$):
\begin{enumerate}[i)]
\item (bilinearity)
 \[[ax + by, z] = a[x,z] + b[y,z]\]
\[[z, ax + by] = a[z,x] + b[z,y]\]
\item (anti-commutation) \[[x,y] = -[y,x] \]
\item (Jacobi identity) \[[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0\]
\end{enumerate}}
If the algebra is over $\mathbb R$ it is a real Lie algebra, and over $\mathbb C$ a complex Lie algebra. We write the name of Lie algebras $\mathfrak l$  in lower case {\bf fraktur} style.

Since Lie algebras are vector spaces, a {\bf subalgebra} $\mathfrak m$  of a Lie algebra $\mathfrak l$ is simply a subspace $\mathfrak m\subseteq \mathfrak l$. An {\bf ideal} is a subalgebra that satisfies $[\mathfrak l, \mathfrak m] \subseteq \mathfrak m$. A {\bf simple} Lie algebra is a non-abelian Lie algebra (non-zero structure constants)  without any proper ideals. A Lie algebra is {\bf semi-simple} if it is a direct sum of simple Lie algebras.

Again the vectors of $\mathbb{R}^3$ with the Lie bracket defined in terms of the cross product, $[\mathbf x, \mathbf y]=\mathbf x \times \mathbf y$, is a simple example of a Lie algebra. However, we usually restrict ourselves to algebras of linear operators where the Lie bracket is the standard commutator $[x,y] \equiv xy-yx$, where the defining properties follow automatically. Thus also explaining the notation that we have used for the binary operator. 

From what we learnt in Section \ref{sec:Lie_groups} the generators $X_i$ of an $d$-dimensional Lie group then span a $d$-dimensional Lie algebra with the commutator as the Lie bracket.\footnote{If you are really paranoid here, you may question wether these generators are guaranteed to be linearly independent. However, since the Lie group is parameterised by $d$ parameters, if the generators were not linearly independent we should be able to remove one of the parameters, leading to a contradiction. Stop it!} Since this is also a vector space, any element $X$ of the algebra can be written in terms of the generators $X_i$, as $X=a_iX^i$. This is called the {\bf tangent space} of the group at the identity. The differences between choices in the definition of the generators and the normalisation of the structure constants disappear as they can be absorbed into a rescaling and linear combinations of the basis $\{X_i\}_{i=1}^d$, this makes the Lie algebra of the group {\it unique}.

However, the reverse is not true. There can be multiple Lie groups that have the same algebra. The often quoted example is the groups $SO(3)$ and $SU(2)$ where we have seen that the commutator of the generators is the same up to constants, so the algebra is the same, we say $\mathfrak{so(3)}\cong \mathfrak{su(2)}$. However, while closely related, the groups are not isomorphic. 


%%%
\subsection{Representations of a Lie algebra}
\label{sec:algebra_rep}

We saw in the previous subsection that -- just as for groups -- we have many different way of writing down a single Lie algebra. We then need a formal definition of representations of Lie algebras.
\df{A {\bf representation} $\pi$ {\bf of a Lie algebra} $\mathfrak l$ on a vector space $V$  is a map
\[\pi: \mathfrak{l} \to \mathfrak{gl}(V), \]
where $ \mathfrak{gl}$ is the Lie algebra of all linear maps $V\to V$ (endomorphisms) with its bracket defined as the composition $[r,s]\equiv r\circ s-s\circ r$, for all $r,s\in \mathfrak{gl}$, and where the map $\pi$ is homomorphic (structure preserving) for the Lie algebra bracket so that for all $x,y\in \mathfrak{l} $,
\[ \pi([x,y]) =\pi(x)\circ\pi(y)-\pi(y)\circ\pi(x). \]
}
Again this is an abstract definition that is actually simple than it looks. For finite dimensional vector spaces $\mathfrak{gl}$ is the Lie algebra of the corresponding general linear matrix group, just as the definition of representations of groups was a map to the general linear group $GL(n)$. This algebra of $GL(n)$, called $\mathfrak{gl}(n)$, can be written up in terms of $n\times n$ matrices and their commutators, and in this case the multiplication operation $\circ$ is just matrix multiplication in hiding. One word of warning here though, while the group $GL(n)$ contained invertible matrices, there is no guarantee that its generators are invertible matrices, {\it e.g.} we saw earlier that the generators of $SO(3)$ were the matrices $L_i$ in Eq~(\ref{eq:SO3_generators}) that are not invertible since $\det L_i=0$.

What makes this definition of representations even simpler for Lie algebras is that {\bf Ado's theorem}~\cite{Ado:1935} proves that every finite-dimensional Lie algebra has a faithful (isomorphic to the Lie algebra) representation on a finite-dimensional vector space.\footnote{For a modern version of a proof of Ado's theorem, see Terence Tao's blog \url{https://terrytao.wordpress.com/2011/05/10/ados-theorem/}.} In other words, the Lie algebras for the groups we most care about, Lie groups with a finite number of parameters and thus finite number of generators, is guaranteed to have representations using matrices.

As for groups, we can talk about irreducible representations, irreps, of the Lie algebras, which are representations such that $V$ has no proper invariant subspace under the representation, meaning that we cannot construct proper sub-representations.
% Maybe say something about similarity transforms between different Lie algebras 
An important result for representations of Lie groups is {\bf Weyl's complete reducibility theorem} which says that the any representation of a finite-dimensional semi-simple Lie algebra  on a finite-dimensional space is isomorphic to a direct sum of irreducible representations. This means that the classification of the representations of semi-simple Lie algebras can be done in a very systematic way. In particular, every semi-simple Lie algebra is a subalgebra of $\mathfrak {sl}$, the Lie algebra of the special linear group, and all our matrix groups have simple or semi-simple Lie algebras.
%TODO: extend to semi-simple representation theory here

In Sec.~\ref{sec:generators} we saw the two sets of generators of the Lie algebras $\mathfrak{su(2)}$ and $\mathfrak{so(3)}$, derived from the defining matrix representations of the groups that have different dimensions. Since this is the same algebra this shows two distinct representations of the same algebra. 


%%%
\subsection{Exponential map}
\label{sec:expmap}
As we discussed in Section \ref{sec:Lie_groups}, the generators describe the local structure of the group near the identity element. We can now finally look at how the group (and matrix representation) is reconstructed from the algebra. For this we use what is called the {\bf exponential map}. 
\df{The {\bf exponential map} from a finite-dimensional Lie algebra $\mathfrak l$ of a group $G$ to $G$ is defined by $\exp:\mathfrak l\to G$, where for $X\in\mathfrak l$ we get the element $g\in G$ given by
\begin{equation}
g=\exp(iX)\equiv\sum_{n=0}^{\infty}\frac{(ia_iX^i)^n}{n!}.
\end{equation}
Here the $X_i$ are the generators of $G$ and thus members of the Lie algebra, and $X=a_iX^i$.
}
By Ado's theorem any (finite-dimensional) Lie algebra can be represented by matrices, in which case the infinite sum in this definition is nothing more than the formal series definition of the matrix exponential introduced in Sec.~\ref{sec:unitary_groups}. However, we can also use the exponential map for differential representations where the $X_i$ are differential operators, and for infinite-dimensional Lie groups, but the technical aspects why this extension works is a little beyond the scope of these notes.

We have already seen multiple example of the exponential map construction, but we may ask, why use the exponential? One reason is that it gives us a natural expansion of the group around the identity element. With the parameters $\mathbf a=0$ the resulting matrix is the identity matrix, which means that the group element is the identity element $g=e$. Similarly, for an infinitesimal $d\mathbf a$ step away from the parameters of the identity we have the group elements near the identity, $g=I+ida_iX^i$ that we used to derive the generators. And now, if we apply a small finite step  $a_i$ away from the identity $g=I+i\frac{a_i}{n}X^i$ a total of $n$ times we will get
\[g=\left(I+i\frac{a_i}{n}X^i\right)^n. \]
By the limit definition of the exponential, with smaller and smaller steps letting $n\to\infty$ this is the exponential map
\[g=\lim_{n\to\infty}\left(I+i\frac{a_i}{n}X^i\right)^n=\exp( ia_iX^i). \]

We should now ask two important questions, is the structure of these elements $g$ actually a group, and is it (isomorphic to) the group that gave the Lie algebra? For the first question we may check the group definition. The exponential map does give an identity element which is just the identity matrix. Associativity similarly holds because we are using matrices and matrix multiplication. There is indeed an inverse of an element $g=\exp{(iX)}$, which is $g^{-1}=\exp{(-iX)}$, since by the properties of matrix exponentiation we have $gg^{-1}=\exp{(iX)}\exp{(-iX)}=I$. The tricky point is whether group multiplication is closed.

If we have two elements from the map $g=\exp{(iX)}$ and $g'=\exp{(iY)}$, then $gg'=\exp{(iX)}\exp{(iY)}$, and we would like to show that  $\exp{(iX)}\exp{(iY)}=\exp{(iZ)}$, where $Z\in\mathfrak l$. However, unless $X$ and $Y$ commute we cannot  easily join them in the same exponential. We here use the {\bf Baker--Campbell--Haussdorff  formula},
\begin{equation}
Z=X+Y+\frac{1}{2}[X,Y]+\frac{1}{12}[[X,Y],Y]-\frac{1}{12}[[X,Y],X]+\ldots,
\label{eq:BKH_matrices}
\end{equation}
where the omitted terms involve Lie brackets of four or more elements, and where every element in the series contains the commutator $[X,Y]$. If $X$ and $Y$ are close enough to the zero in $\mathfrak l$ (corresponding to the identity element in $G$) then this series can be shown to converge to an element in $\mathfrak l$.\footnote{This formula holds even if the elements in the Lie algebra ${\mathfrak l}$ are not represented as finite-dimensional matrices.} Thus the exponential constructs a group at least locally around the identity element.

The question of whether the exponential map reaches all of the members of the group, {\it i.e.}\ that it is surjective on $G$, depends on the properties of the group. We certainly have counter examples, since the groups $SO(3)$ and $SU(2)$ have the same Lie algebra and thus the same exponential map, but are different groups, the map cannot reach all of the elements of both groups. What we do know is that locally, meaning sufficiently close to the identity group element, the exponential map generates the group.

Let us end this subsection by returning to some of our examples in view of what we now know about the exponential map. For $U(1)$ we saw that we could write a generic group member as $e^{i\theta}$. Comparing to the exponential map we see that the single generator must here simply be 1, while $\theta$ is the parameter. The Lie algebra is then a very simple one-dimensional vector space $\mathbb R$ where the multiplication is ordinary arithmetic multiplication. Similarly, for the translation group $T(1)$ we saw that a group member could be written as $e^{a\frac{\partial}{\partial x}}=e^{ia(-i\frac{\partial}{\partial x})}$. Thus the generator is the differential operator $P=-i\frac{\partial}{\partial x}$ and again the vector space of the Lie algebra is one dimensional.

For $SU(2)$ the generators taken from the fundamental representation $\mathbf 2$ were proportional to the Pauli matrices $X_i= \frac{1}{2}\sigma_i$, and the exponential map generating a group member $U$ is thus, as we alluded to earlier, $U=e^{i\alpha_iX^i}$, where $\alpha_i$ are the parameters. For $SO(3)$ the (Hermitian) generators taken from the fundamental representation $\mathbf 3$ are the matrices $Y_i=-iL_i$, where the $L_i$ were given in (\ref{eq:SO3_generators}), and the exponential map is $O=e^{i\alpha_iY^i}$.
Since $SU(2)$ and  $SO(3)$ have the same algebra for $X_i$ and $Y_i$, but the groups are {\it not} isomorphic,  in what sense, if any, does these exponential maps generate different elements? The answer to that is that $SU(2)$ is a {\bf double cover} of $SO(3)$, it has double the number of elements. There exists a two-to-one group homomorphism $f:SU(2)\to SO(3)$. This can be represented by $U(\alpha_1,\alpha_2,\alpha_3)\to O(\alpha_1,\alpha_2,\alpha_3)$. To see that this is two-to-one, assume that $\alpha_1=\alpha_2=0$. Then $O(0,0,\alpha_3)=O(0,0,\alpha_3+2\pi)$ since a rotation by an extra $2\pi$ does nothing. For $SU(2)$ we instead have
\[
U(0,0,\alpha_3)=\left[\begin{matrix} e^{i\frac{\alpha_3}{2}} & 0 \\ 0 & -e^{i\frac{\alpha_3}{2}} \end{matrix}\right] \quad \text{and}\quad U(0,0,\alpha_3+2\pi)=\left[\begin{matrix} e^{i\frac{\alpha_3}{2}+i\pi} & 0 \\ 0 & -e^{i\frac{\alpha_3}{2}+i\pi} \end{matrix}\right]=-U(0,0,\alpha_3)
\]
Thus the $SU(2)$ matrices has a period $4\pi$ versus the period $2\pi$ for $SO(3)$.\footnote{The argument can be made more formal and independent of the parameterisation, but is to extensive to repeat here.} Notice that this means that the three-dimensional representation of the Lie algebra coming from the generators of $SO(3)$ does not generate all the elements of $SU(2)$, while the two-dimensional one does.
% TODO: Better description of double cover in 3.2 https://www.weizmann.ac.il/particle/perez/Courses/QMII18/TA7.pdf

Beyond the fundamental representation $SU(2)$ has irreducible representations in all dimensions $\mathbf 3$, $\mathbf 4$, $\mathbf 5$, \ldots. This is not true in general, for example $SU(3)$ has irreducible representations only in some dimensions, starting with $\mathbf 3$, $\mathbf 6$, $\mathbf 8$, and $\mathbf{10}$, \ldots.

From the exponential map and the properties of matrix exponentiation it immediately follows that the generators $T^a$ for all groups $SU(n)$ are matrices with zero trace. To see this write an element in $SU(n)$ as $U=\exp{(i\alpha_aT^a)}$, then by the defining group property $\det U=1$, we have $\det U=\exp{(\Tr[{i\alpha_aT^a}}])=\exp{(i\alpha_a\Tr{T^a}})$ which is only one if $\Tr T^a=0$ for all $a$.  

We saw in Sec.~\ref{sec:unitary_groups} that we could construct unitary matrices from Hermitian ones by matrix exponentiation, but in fact for unitary groups the generators need to be Hermitian. To see this let a group member be $U=\exp(ia_iM^i)$. Then $U^\dagger = \exp(ia_iM^i)^\dagger=\exp(-ia_iM^{i\dagger})$,  but for a unitary matrix $U^\dagger = U^{-1}=\exp(-ia_iM^i)$, thus $M^{i\dagger}=M^i$ and the generators $M_i$ are Hermitian.
%TODO: Add tracelessness for $SU$. If $U=\exp(ia_iM^i)$ then $\det U = \exp{ia_i\Tr[M^i]}$ and if $\det U =1$ then we must have $\Tr M_i =0$ for all $i$.


%%%
\subsection{Adjoint representations}
\label{sec:adjoint_reps}
The definition of the structure constants in Sec.~\ref{sec:structure_constants} allows us to introduce another representation, the {\bf adjoint representation} of the algebra -- and by the exponential map the adjoint representation of the group -- where the representation of the algebra consists of the matrices $M_i$ with elements:
\[(M_i)_j^{~k} = -iC_{ij}^{~~k},\]
where $C_{ij}^{~~k},$ are the structure constants. If the dimension of the Lie algebra is $n$, then this is the number of generators, and it also means that the dimension of the matrices $M_i$ is $n\times n$. 
% TODO: Comment on when index position is important (not for semi-simple compact groups). Source?
To prove that this is actually a representation, we need to check that the mapping from the elements of the algebra $X_i$ to the matrices $M_i$ satisfies the homomorphism criteria of the Lie algebra representations. From the Jacobi identity of the algebra it follows that
\begin{equation}
[M_i, M_j] = iC^{~~k}_{ij}M_k,
\label{eq:adjoint_rep_commutator}
\end{equation}
meaning that the matrices of the adjoint representation fulfils the same commutator relationship as the fundamental (generators). As a direct consequence the $M_i$ form a representation.

Note that the dimension $n$ of the matrices in the adjoint representation for $\mathfrak so(m)$ and $\mathfrak su(m)$, which is equal to the number of independent parameters, is $n=m(m-1)/2$ and $n=m^2-1$, respectively. For the matrices in the representation derived from the defining representation of the group the dimensions is the same as the dimension of the defining matrices, $m$. This means that in most cases the matrices in the adjoint representation is larger, with the exception of  $\mathfrak{so}(2)$ and  $\mathfrak{so}(3)$.

We can now briefly return to the $SU(2)$ and $SO(3)$ examples. These groups have the same structure constants because they have the same commutators between generators. Thus the adjoint representations of their algebras are the same. With structure constants $C^{~~k}_{ij}=\epsilon_{ij}^{~~k}$ we get the elements of the adjoint matrices $(M_i)_{jk} =-i\epsilon_{ijk}$, which gives $M_i=-iL_i$, with the $L_i$ being the matrices in (\ref{eq:SO3_generators}). 

From the adjoint representation of the Lie algebra we can construct  the adjoint representations of the Lie group. For example the adjoint representation of  $\mathfrak{su}(2)$ or $\mathfrak{so}(3)$ has the exponential map $\exp{(ia_i M^i)}=\exp{(a_i L^i)}$. This happens to generate the $SO(3)$ fundamental representations, but this is not generally the case, it is ``accidental'' that the dimensions of the fundamental representation of $SO(3)$, $n=3$, and the dimension of its adjoint representation $n=3(3-1)/2=3$ is the same.


%%%
\subsection{Dual representations}
\label{sec:dual_reps}

Every representation of a Lie algebra on a vector space $V$ (and every representation of a group) has a {\bf dual representation} (also known as a {\bf contragredient representation}) that is a representation on the {\bf dual vector space} $V^*$. The definition of a dual vector space is somewhat abstract, it is formally the space consisting of all linear maps $\phi:V\to K$, where $K$ is the field of $V$. Since the map takes members of $V$ and maps them linearly to the field $K$, for a finite-dimensional vector space we must be able to write any member of $V^*$ in terms of a row-vector over the same field, so that if $\mathbf v\in V$ and $\mathbf w^T \in V^*$, the map is given by the matrix multiplication $\mathbf w^T \mathbf v$ which gives a scalar. This means that in practice  $V^*$ consists of the corresponding row-vectors to the column vectors in $V$.\footnote{It is slightly unfortunate and confusing that the symbol for the dual space uses a star since this seems to imply complex conjugation, where instead we are actually talking about transpose, however, as we will see below, the complex conjugation is central to dual representations of unitary groups.}

The definition of the dual representation for a Lie algebra is as follows:\footnote{This definition also applies beyond finite-dimensional vector space with a basis since the transpose operation can be generalised beyond matrices.}
\df{If ${\mathfrak l}$ is a Lie algebra with a representation $\rho$ on a vector space $V$, then the {\bf dual representation} on the dual space $V^*$ is $\rho^*$ given by
\[ \rho^*(x) = -\rho(x)^T, \]
for all $x\in\mathfrak l$.}
To see why this makes sense we need to look at the exponential map of the dual representation. If $X_i$ is a generator in the representation $\rho$, then the dual representation $\rho^*$ tells us to use instead $-X_i^T$. In the exponentiation the group element $g=\exp(iX_i)$ becomes the group element $\exp(-iX_i^T)=\exp(-iX_i)^T=(g^{-1})^T$ with the dual representation. This is the only sensible representation that fulfils the group multiplication properties as matrices acting on row-vectors.
% For this to be a group representation we must have that a product of two group elements $(g_1^{-1})^T(g_2^{-1})^T$ 

For unitary groups like $U(n)$ or $SU(n)$ we have matrix representations of the Lie algebra with Hermitian generators $M_i=M_i^\dagger$,  generating the members in the group as $U=\exp(ia_iM^i)$. From the definition of the dual representation the group members generated in the dual representation are then
\[ U=\exp(-ia_iM^{iT})=\exp(-ia_i(M^{i\dagger})^*)=\exp(-ia_iM^{i*}), \]
meaning that the Lie algebra in the dual representation can be written in terms of Hermitian matrices $M_i'=-M_i^*$. As a check we can see that $M_i'$ indeed fulfils the same algebra:
\[ [M_i',M_j']=[M_i^*,M_j^*]=-iC_{ij}^{~~k}M_k^*=iC_{ij}^{~~k}M_k',\] 
which is true only if the structure constants $C_{ij}^{~~k}$ are real.\footnote{You can take this as a proof that the structure constants are real if you prove that the dual representation is actually a representation, or you can go the other way and prove that this is indeed a representation, if you can first show that the structure constants of unitary groups are real.}
For the matrix groups we denote the dual representations using a bar, so that for example the dual of a representation $\mathbf 2$ is $\mathbf {\bar 2}$. The dual representations play a special role in quantum field theories, as the known fermions and anti-fermions transform under a representation of the gauge group and its dual, respectively.

%If a finite-dimensional representation of a group is irreducible, then one can show that the dual representation is also irreducible. 
For some groups, such as $SU(2)$, the original and dual representations can be shown to be isomorphic. However, this is not in general the case. For $SU(3)$ the dual representations $\mathbf {\bar 3}$, $\mathbf {\bar 6}$, and $\mathbf{\overline{10}}$, are not isomorphic to $\mathbf 3$, $\mathbf 6$, and $\mathbf{10}$. On the other hand, $\mathbf 8$ and $\mathbf {\bar 8}$ are isomorphic.\footnote{However, the dual of the dual of any representation is always isomorphic to the original representation.} This becomes important in the Standard Model as the quarks transform under the $\mathbf 3$ representation of the $SU(3)_c$ gauge group, and the anti-quarks under $\mathbf {\bar 3}$.





%%%%%%%%%
\section{Exercises}
%%%%%%%%%
\begin{Exercise}[]
Show that for the translation group $T(1)$ the identity element is $T_0$ and that $T_a^{-1} = T_{-a}$, and use this to show that $T(1)$ is group.
\end{Exercise}

\begin{Answer}
Given the group multiplication definition the identity element must be $e=T_0=1$ since $T_a\circ T_0=T_{a+0}=T_a$. Let $T_a^{-1} =T_b$. Since $T_a \circ T_b=T_{a+b}=1$, we find $b=-a$. (This does not show that the inverse is unique, but this is not a requirement.) We have now demonstrated the required existence of an identity element (iii) and an inverse (iv) for $T(1)$ as required by the group definition (the order of operations can obviously be reversed). The closure of the multiplication operation (i) is true by its definition. The associativity (ii) can be demonstrated as follows $(T_a\circ T_b)\circ T_c=T_{a+b}\circ T_c=T_{a+b+c}=T_{a}\circ T_{b+c}=T_a\circ (T_b\circ T_c)$.
\end{Answer}

\begin{Exercise}[]
Show that the set of complex unitary $n\times n$ matrices $U$ with matrix multiplication is a group.
\end{Exercise}
\begin{Answer}
We need to demonstrate the four group requirements. We have closure since the product of two such unitary matrices $U_1$ and $U_2$ is a unitary matrix: $(U_1U_2)^\dagger U_1U_2= U_2^\dagger U_1^\dagger U_1U_2= U_2^\dagger U_2=I$. Matrix multiplication is always associative. There exists an identity among the matrices since the identity matrix is unitary: $I^\dagger I = II=I$. There exist an inverse $U^{-1}=U^\dagger$ for every $U$, since the matrix $U^\dagger$ is unitary if $U$ is: $(U^\dagger)^\dagger U^\dagger= UU^\dagger=I$.
\end{Answer}

\begin{Exercise}[]
What are the elements of $O(1)$? Can you find a group that it is isomorphic to?
\end{Exercise}
\begin{Answer}
Since $O(1)$ consists of $1\times1$ matrices, over $\mathbb R$ we suppose, also known as real numbers, the orthogonality requirement means that the transpose of that number (which is the number itself) multiplied by the number should be 1. There are two such reals, 1 and -1. The group thus has two elements. Since matrix multiplication in this case is just normal multiplication we have re-found the group $\mathbb Z_2$.
\end{Answer}

\begin{Exercise}[label=ex:Un_parameters]
Show that the $U(n)$ and $SU(n)$ groups have $n^2$ and $n^2-1$ independent (real) parameters, respectively. {\it Hint:} Consider the fact that $M=U^\dagger U$ is a hermitian matrix, {\it i.e.}\ $M^\dagger=M$.
\end{Exercise}
\begin{Answer}
An $n\times n$ matrix $U$ over $\mathbb C$ has $n^2$ complex or $2n^2$ real parameters. The hermitian matrix $M=U^\dagger U$ has $n$ real elements on the diagonal
and $(n^2-n)/2$ complex elements above and below the diagonal. Because $M^\dagger=M$ all the $(n^2-n)/2$ elements below the diagonal are given by the complex conjugate the corresponding elements above the diagonal. Since $U$ is unitary $M=I$. That all the $n$ real diagonal elements of $M$ is equal to one gives $n$ restrictions ($n$ real equations) for the elements of $U$. Further, all $(n^2-n)/2$ complex elements above the diagonal are zero, which gives $(n^2-n)/2$ complex equations, which means $n^2-n$ real equations. For the terms below the diagonal we do not obtain any new equations. As a result the free parameters for the $U(n)$ matrices are $2n^2-n-(n^2-n)=n^2$.
For $SU(n)$ we must use the additional requirement $\det U=1$. Because $1=\det M=\det (U^\dagger U)=\det U^\dagger \det U=|\det U|^2$, we see that $\det U$ must be a phase factor, and that the requirement $\det U=1$ only gives one new condition (equation). Thus there are $n^2-1$ independent real parameters in $SU(n)$.
\end{Answer}

\begin{Exercise}[]
Show that Eq.~(\ref{eq:SU2_parameterisation}) is a parametrisation of $SU(2)$.
\end{Exercise}
\begin{Answer}
A matrix in $SU(2)$ has $2^2-1=3$ free parameters. Let us start with a generic $2\times2$ complex valued matrix
\[ U=\left[ \begin{matrix} \alpha & \beta \\ \gamma & \delta \end{matrix} \right]. \]
For this to be unitary we must have 
\[ U^\dagger U=\left[ \begin{matrix} \alpha^* & \gamma^* \\ \beta^* & \delta^* \end{matrix} \right]\left[ \begin{matrix} \alpha & \beta \\ \gamma & \delta \end{matrix} \right] 
=\left[ \begin{matrix} |\alpha|^2+|\gamma|^2 & \alpha^*\beta+\gamma^*\delta \\ \beta^*\alpha+\delta^*\gamma & |\beta|^2+|\delta|^2 \end{matrix} \right]
=\left[ \begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix} \right].\]
Then from the off-diagonal, $\delta^*\gamma=-\beta^*\alpha$ and $\gamma^*\delta=-\alpha^*\beta$ which gives $|\delta|^2|\gamma|^2= |\alpha|^2 |\beta|^2$, and inserting from the diagonal we have $(1-|\beta|^2)|\gamma|^2= (1-|\gamma|^2) |\beta|^2$ which can be solved for $|\gamma|^2= |\beta|^2$. Similarly from the diagonal $|\delta|^2=|\alpha|^2$. We can then generically  write $\gamma=e^{i\theta}\beta$ and $\delta=e^{i\phi}\alpha$, where $\theta$ and $\phi$ are two phases. The diagonal requirement is then completely fulfilled if $|\alpha|^2+|\beta|^2=1$ and the matrix is
\[ U=\left[ \begin{matrix} \alpha & \beta \\ e^{i\theta}\beta & e^{i\phi}\alpha \end{matrix} \right]. \]
The determinant then requires $\alpha e^{i\phi}\alpha-\beta e^{i\theta}\beta=1$ which is fulfilled for a phase $\phi$ that rotates $\alpha$ to  $\alpha^*$ and a phase $\theta$ that rotates $\beta$ to $-\beta^*$. Thus we can write
\[ U=\left[ \begin{matrix} \alpha & \beta \\ -\beta^* & \alpha^* \end{matrix} \right]. \]
\end{Answer}

\begin{Exercise}[]
Show that for a subset $H\subset G,$ if $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$, then $H$ is a subgroup of $G$.
\end{Exercise}
\begin{Answer}
\end{Answer}

\begin{Exercise}[]
Show that if $H$ is a subgroup of $G$, then $h_i \circ h_j^{-1} \in H$ for all $h_i, h_j\in H$.
\end{Exercise}
\begin{Answer}
Since $H$ is a subgroup of $G$ then by point ii) in the definition since $h_j\in H$ we must also have $h_j^{-1} \in H$. With $h_i, h_j^{-1} \in H$ by point i) in definition $h_i  \circ h_j^{-1} \in H$.
\end{Answer}

\begin{Exercise}[label=ex:SUn_normal]
Show that $SU(n)$ is a proper subgroup of $U(n)$ and that $U(n)$ is not simple.
\end{Exercise}

\begin{Answer}
Let $U_i, U_j \in SU(n)$, then 
\[\det(U_i U_j^{-1}) = \det(U_i)\det(U_j^{-1}) = 1.\]
This means that $U_i U^{-1}_j \in SU(n)$. In other words, $SU(n)$ is a proper subgroup of $U(n)$. Let $V \in U(n)$ and $U \in SU(n)$, then $VUV^{-1} \in SU(n)$ because:
\[\det(VUV^{-1}) = \det(V)\det(U)\det(V^{-1}) = \frac{\det(V)}{\det(V)} \det(U) = 1.\]
In other words, $SU(n)$ is also a normal subgroup of $U(n)$, thus $U(n)$ is not simple.
\end{Answer}

\begin{Exercise}
Show any two (left or right) cosets are either disjoint or identical sets.
\end{Exercise}
\begin{Answer}
Let $H$ be a subgroup of $G$ and $g\in G$. For a member of the left coset $f\in gH$ then $gH=fH$ because $f\in gH$ implies that there must exist an $x\in H$ so that $gx=f$. Thus $fH=(gx)H=g(xH)$. Since $H$ is a group this means $xH=H$.  Thus every element in $G$ belong to exactly one left coset. The argument of right cosets is identical.
\end{Answer}

\begin{Exercise}[]
If $H$ is a normal subgroup of $G$, show that its left and right cosets are the same, and show that the set formed of the cosets is a group under an appropriate group operation.
\end{Exercise}

\begin{Exercise}[]
Show that the factors in a direct product of groups are normal groups to the product.
\end{Exercise}

\begin{Exercise}[]
Show that the group $G$ and the group $(G\times H) / H$ are isomorphic.
\end{Exercise}


\begin{Exercise}[]
Show that $U(1)\cong\mathbb{R}/\mathbb{Z}\cong SO(2)$.
\end{Exercise}


\begin{Exercise}[difficulty={2}]
Prove that all unitary representations on Hilbert spaces are completely reducible. {\it Hint:} For Hilbert spaces we always have an inner product defined.
\end{Exercise}
\begin{Answer}
Split the representation space $V=V_1\oplus V_2$, where we assume $V_1$ is a closed subspace of $V$ under the unitary representation $\rho(g)$ (the reducible part). Let $\mathbf v_1\in V_1$ and $\mathbf v_2\in V_2$.  Since $V_1$ is closed $\rho(g)\mathbf v_1 \in V_1$ and thus the inner product $(\rho(g)\mathbf v_1,  \mathbf v_2)=0$.  By unitarity the inner product is also $(\rho(g)\mathbf v_1,  \mathbf v_2)=(\mathbf v_1, \rho(g)^{-1}\mathbf v_2)$, thus $\rho(g)^{-1}V_2 \subset V_2$ and both subspaces are closed, and the space thus completely reducible. 
\end{Answer}


\begin{Exercise}[difficulty={1}]
Use the parameterisation of $SO(2)$ in (\ref{eq:SO2_parameterisation}) to find the generator of the group using generating functions and show that you get the same answer as when using the infinitesimal matrix demonstrated in the notes.
\end{Exercise}


\begin{Exercise}[]
Find the fundamental representation for $SO(3)$.
\end{Exercise}


\begin{Exercise}[difficulty={3},label=ex:SO2_diff_generators]
Find the differential generators for $SO(3)$ and their commutation relationships by studying how a function on $\mathbb R^3$ changes under $SO(3)$ transformations. Show that they satisfy the same commutators as the matrix generators.
\end{Exercise}


\begin{Exercise}[difficulty={3},label=ex:SU2_generators]
Find an expression for the generators of $SU(2)$ and their commutation relationships. {\it Hint:} One answer uses a composition function but this approach has some dangers, try also to derive the properties of the generators from a member of the group an infinitesimal distance from the identity.
\end{Exercise}
\begin{Answer}
A member $U$ of $SU(2)$ fulfils $UU^\dagger=I$ and has $\det U =1$ and can be written as 
\[U=\left[\begin{matrix} a+ib & c+id \\ -c+id &  a-ib \end{matrix}\right],\] 
where $a,b,c,d\in\mathbb R$ and $a^2+b^2+c^2+d^2=1$. Using a composition function here it is tempting to solve for $a=\sqrt{1-b^2-c^2-d^2}$ and let $a,b,c$ be the three free parameters if $SU(2)$. Note that solving for $b$, $c$ or $d$ is not really an option as a requirement for deriving the generators from the composition function is that the zero parameters, here $b=c=d=0$, give the identity element of the group, which is the identity matrix. However, there is a potential issue here that choosing the sign on the square root restricts the parameterisation to apply for only group members with positive real components in the upper left element, meaning that this composition function only works for part of the group. This is another example of a local description of the group.  Other parameterisations exist, however, they may have other problems such as a non-unique zero element (meaning no inverse of the composition function exists).

Fortunately, this parameterisation is enough to give all of the generators of the whole group:
\begin{eqnarray*} 
X_1&=&\left.\frac{\partial U}{\partial  d}\right|_{b=c=d=0}=\left[\begin{matrix} 0 & i \\ i & 0 \end{matrix}\right], \\
X_2&=&\left.\frac{\partial U}{\partial  c}\right|_{b=c=d=0}=\left[\begin{matrix} 0 & 1 \\ -1 & 0 \end{matrix}\right], \\
X_3&=&\left.\frac{\partial U}{\partial  b}\right|_{b=c=d=0}=\left[\begin{matrix} i & 0 \\ 0 & -i \end{matrix}\right].
\end{eqnarray*} 
While these are not the Pauli matrices we might be expecting, they are related to the Pauli matrices by a simple multiplicative factor $\sigma_1=-iX_1$,  $\sigma_2=-iX_2$,  and $\sigma_3=-iX_3$. In fact the matrices we have found are typically used in mathematical texts as the generators of $SU(2)$, but then the exponential map has no factor of $i$. Notice that these matrices are anti-hermitian, while the Pauli matrices are hermitian.

The infinitesimal approach writes the group member as $U=e^{ida_iX_i}\simeq I+ida_i X_i$, and applying the unitarity requirement 
\[UU^\dagger\simeq(I+ida_i X_i)(I+ida_i X_i)^\dagger=I+ida_i(X_i-X_i^\dagger)+\ldots,\]
means that a necessary condition on the generators $X_i$ is that they are hermitian $X_i=X_i^\dagger$. The general form of the generators is thus restricted to
\[X=\left[\begin{matrix} \alpha & \gamma \\ \gamma^* &   \beta  \end{matrix}\right],\] 
where  $\alpha,\beta\in\mathbb R$  and $\gamma\in\mathbb C$. The property of the determinant can be found as follows (see Exercise~\ref{ex:expmap_prop}):
\[\det U =\det e^{ida_iX_i}=e^{\tr{ida_iX_i}}=e^{ida_i\tr{X_i}}.\]
This means that for the determinant to be one, each trace must be zero, meaning that the generators are traceless so that the general form of the generators is restricted to
\[X=\left[\begin{matrix} \alpha & \gamma \\ \gamma^* &   -\alpha  \end{matrix}\right].\]
From the requirement $\alpha^2+|\gamma|^2=1$ we can recreate the three Pauli matrices by the three choices: 1) $\alpha=1$, $\gamma=0$, 2) $\alpha=0$, $\gamma=1$, and 3) $\alpha=0$, $\gamma=-i$. Naturally, there is a continuum of equivalent expressions for the generators from other compatible choices. 

This approach emphasises two important properties of the generators. Firstly that the generators are hermitian. Since the generators will function as operators in a QM of QFT setting, this is very desirable. Second, the generators are traceless.  This will often be a calculational advantage.
\end{Answer}


\begin{Exercise}[]
What are the structure constants of SU(2)?
\end{Exercise}


\begin{Exercise}[]
Find the adjoint representation for $SU(2)$. Compare this to the fundamental representation of $SO(3)$.
\end{Exercise}



\begin{Exercise}[]
Let $A$ be an algebra based on a finite-dimensional vector space over a field $F$ ,with a basis $B=\{b_i | i=1,\ldots,n\}$. Show that the multiplication of elements in $A$ is completely determined by the $n^2$ products $b_ib_j$ for each pair of basis vectors in $B$. 
\end{Exercise}


\begin{Exercise}[]
Let $V$ be a finite-dimensional vector space over a field $F$ with a basis $B=\{b_i | i=1,\ldots,n\}$. Let  $\{c_{rst}| r,s,t=1,\ldots,n\}$ be a collection of $n^3$ elements in $F$. Show that there exists one, and only one, multiplication operation on $V$ so that $V$ is an algebra over $F$ under this multiplication and
\[ b_rb_s = c_{rst}b_t,\]
for every pair of basis vectors in $B$. The elements $c_{rst}$ are the structure constants of the algebra.
\end{Exercise}


\begin{Exercise}[]
Show that $\mathbb{R}^3$ with the binary operator $[\mathbf x, \mathbf y]=\mathbf x \times \mathbf y$ is a Lie algebra.
\end{Exercise}

\begin{Exercise}[]
Show Eq.~(\ref{eq:adjoint_rep_commutator}).
\end{Exercise}

\begin{Exercise}[]
Let $A_i$ be the generators of the group $G$ and $B_j$ be the generators of group $H$. Explain in what sense the $A_i$ and $B_j$ are generators of the direct product group $G\times H$ and show that $[A_i,B_j]=0$.
\end{Exercise}


\begin{Exercise}[title={Properties of matrix exponentiation},label={ex:expmap_prop},label=ex:expmapprop]
Prove the following useful properties of matrix exponentiation. $A$ and $B$ are matrices.
\begin{enumerate}
\item If $A$ and $B$ commute, $e^{A+B}=e^Ae^B$.
\item If $B$ has an inverse, $e^{BAB^{-1}}=Be^AB^{-1}$.
\item $e^{A^*}=(e^A)^*$
\item $e^{A^T}=(e^A)^T$
\item $e^{A^\dagger}=(e^A)^\dagger$
\item $e^{-A}=(e^A)^{-1}$
\item $\det e^A=e^{\tr{A}}$.
\end{enumerate}
\end{Exercise}



\end{document}
